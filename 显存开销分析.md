# 通用语种攻击新增功能的显存开销分析

本文梳理近期在通用语种对抗扰动训练中新增的功能（批量语言预测、日志输出、显存优化等）对显存占用的影响，并结合代码指出主要来源与当前缓解手段。

## 主要显存开销来源

1. **批量语言预测堆叠 logits/probs**：
   - `compute_forward_lang` 不再仅处理首条音频，而是遍历整个 batch，将每条音频的语言 logits/概率都堆叠后返回。【F:lang_attack.py†L30-L59】
   - 概率矩阵已改为保留在 CPU（不再 `.to(device)`），避免在 GPU 上额外占用 `batch_size × num_lang` 的显存；logits 仍留在 GPU 参与交叉熵计算。

2. **训练阶段的 PGD 缓冲与记录**：
   - PGD 内循环为每个 batch 创建 `delta_batch`、`r_batch` 并开启梯度，显存与批大小、序列长度成正比；`train_history` 仅保存标量到 Python list，对显存影响极小。【F:universal_lang_attack.py†L273-L458】
   - 新增的滑动平均与 SNR 计算均在标量上操作，不会显著增加显存。

3. **语言预测诊断日志**：
   - `_log_language_predictions` 在 `torch.no_grad()` 下额外跑一次前向，并为对抗/干净语音各生成一批 logits；由于使用同一批数据且不保留梯度，开销主要是前向临时激活，退出作用域即释放。【F:universal_lang_attack.py†L163-L227】
   - 打印前会把预测 token 解码为字符串，存入 Python list，对 GPU 占用可忽略。

4. **验证阶段的显存优化**：
   - 成功率评估已显式包裹 `torch.no_grad()`，避免在验证时累积计算图，从而减少峰值显存压力。【F:universal_lang_attack.py†L474-L552】

## 对比与建议

- 与早期版本相比，最大的新增显存占用来自“批量堆叠语言概率”，尤其在 `batch_size` 较大或语言类别多时明显。当前已将 `language_probs` 固定在 CPU，若仍需进一步节省，可完全移除概率矩阵，仅保留 logits 参与训练。
- 日志相关改动（训练历史、语言预测打印）主要占用 CPU 内存，且在无梯度上下文中执行，对显存影响轻微。
- 评估环节已使用 `no_grad`，训练时若仍遇到 OOM，可酌情降低 `batch_size` 或关闭 `log_lang_pred_every`，以减少额外前向的激活开销。

## 最新 OOM 报错路径解析（语言识别阶段）
- 回溯显示 OOM 发生在 `detect_language_with_gradients` 的 `model.encoder` → `qkv_attention` 的 softmax 上，属于语言识别前向的显存峰值。【F:whisper_with_gradients.py†L38-L59】
- 触发场景：`universal_lang_attack` 在每个 PGD 步内按 `batch_size` 切片后的整块（未设置 `lang_microbatch_size` 时即整个 batch）调用 `compute_forward_lang`，该函数为每条语音独立跑一遍 encoder 并把所有 logits 堆叠后再返回。【F:universal_lang_attack.py†L320-L369】【F:lang_attack.py†L30-L62】在反向传播前，这些 per-sample 计算图同时常驻显存，批量越大、音频越长或 `nb_iter` 越多，峰值越高。
- 当 batch=8、nb_iter=10、eps=0.008 时，加载保存点后首个 PGD 步就需要为 8 条 30s 语音构建语言识别计算图，Whisper-small 的多头注意力在全精度 softmax 前将 qk 转为 float32，会额外放大显存占用，最终在 qkv_attention softmax 处分配 104MB 失败。
- 缓解建议：
  - 在命令或 YAML 设置 `lang_microbatch_size`（如 1 或 2），让语言识别前向分块反传，避免一次堆满整个 batch 的计算图；
  - 进一步减小 `batch_size` 或缩短 `nb_iter`/音频长度，可线性降低该段显存峰值；
  - 保持 `lang_autocast=True` 使用混合精度，必要时仅在语言识别阶段单独降精度或改用 `torch.cuda.empty_cache()` 释放碎片，但核心还是控制同时参与计算图的样本数。
